{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "TensorflowOperatorJobTITAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD6mKgqlz6vv"
      },
      "source": [
        "# Training on CHURN Dataset using Tensorflow Operator\n",
        "\n",
        "## Prerequisites\n",
        "Before we proceed, let's check that we're using the right image, that is, [TensorFlow](https://www.tensorflow.org/api_docs/) is available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd3gyUdjz6v-"
      },
      "source": [
        "#! pip3 list | grep tensorflow \n",
        "! pip3 install --user tensorflow==2.4.0\n",
        "! pip3 install --user ipywidgets nbconvert\n",
        "!python -m pip install --user --upgrade pip\n",
        "!pip3 install pandas scikit-learn keras tensorflow-datasets --user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvXsYoXvz6v_"
      },
      "source": [
        "To package the trainer in a container image, we shall need a file (on our cluster) that contains the code as well as a file with the resource definitition of the job for the Kubernetes cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgvjX5euz6wA"
      },
      "source": [
        "TRAINER_FILE = \"tfjobtitan.py\"\n",
        "KUBERNETES_FILE = \"tfjob-titan.yaml\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Hxv7-Jz6wA"
      },
      "source": [
        "We also want to capture output from a cell with [`%%capture`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture) that usually looks like `some-resource created`.\n",
        "To that end, let's define a helper function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18NRbcdVz6wB"
      },
      "source": [
        "import re\n",
        "\n",
        "from IPython.utils.capture import CapturedIO\n",
        "\n",
        "\n",
        "def get_resource(captured_io: CapturedIO) -> str:\n",
        "    \"\"\"\n",
        "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
        "\n",
        "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
        "    :return: Name of the Kubernetes resource\n",
        "    :rtype: str\n",
        "    :raises Exception: if the resource could not be created\n",
        "    \"\"\"\n",
        "    out = captured_io.stdout\n",
        "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
        "    if matches is not None:\n",
        "        return matches.group(1)\n",
        "    else:\n",
        "        raise Exception(f\"Cannot get resource as its creation failed: {out}. It may already exist.\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjs3GWI5z6wB"
      },
      "source": [
        "## How to Load and Inspect the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "_svJFux5z6wC",
        "outputId": "858cc019-2078-4188-a768-0f213cdb2a85"
      },
      "source": [
        "import pandas as  pd\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Day2/KubeflowComponentsAndPipeline/Labs/6_minio/titanic/datasets/train.csv\")\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBi20f0Vz6wE"
      },
      "source": [
        "## How to Train the Model in the Notebook\n",
        "We want to train the model in a distributed fashion, we put all the code in a single cell.\n",
        "That way we can save the file and include it in a container image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "trainer_code"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18iIGU8Hz6wF",
        "outputId": "3aede0a8-e2fb-4d92-9c6d-df25a7225976"
      },
      "source": [
        "%%writefile $TRAINER_FILE\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# splitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Standardization - feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# data encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_datasets_unbatched():\n",
        "  data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Day2/KubeflowComponentsAndPipeline/Labs/6_minio/titanic/datasets/train.csv\")\n",
        "\n",
        "  #preprocessing\n",
        "  data['relatives'] = data['SibSp'] + data['Parch']\n",
        "  data.loc[data['relatives'] > 0, 'not_alone'] = 0\n",
        "  data.loc[data['relatives'] == 0, 'not_alone'] = 1\n",
        "  data['not_alone'] = data['not_alone'].astype(int)\n",
        "\n",
        "  # drop columns with high cardinality\n",
        "  data = data.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\n",
        "\n",
        "  #dealing with missing data in cabin feature\n",
        "  deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
        "\n",
        "  data['Cabin'] = data['Cabin'].fillna(\"U0\")\n",
        "  data['Deck'] = data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
        "  data['Deck'] = data['Deck'].map(deck)\n",
        "  data['Deck'] = data['Deck'].fillna(0)\n",
        "  data['Deck'] = data['Deck'].astype(int)\n",
        "  # we can now drop the cabin feature\n",
        "  data = data.drop(['Cabin'], axis=1)\n",
        "\n",
        "  #dealing with missing data in age feature\n",
        "  data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].mean())\n",
        "\n",
        "  #dealing with missing data in emabrk feature\n",
        "  # fill with most common value\n",
        "  common_value = 'S'\n",
        "  data['Embarked'] = data['Embarked'].fillna(common_value)\n",
        "\n",
        "  # encode categorical variables\n",
        "  data = pd.get_dummies(data)\n",
        "\n",
        "  X=data.drop(\"Survived\",axis=1)\n",
        "  y=data.Survived\n",
        "\n",
        "    \n",
        "  # split the data\n",
        "  X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 10)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "  train = train_dataset.cache().shuffle(100).repeat()\n",
        "  return train, test_dataset\n",
        "\n",
        "\n",
        "def model(args):\n",
        "  model = models.Sequential()\n",
        "  model.add(Dense(units =20, activation='relu', input_dim=13))\n",
        "  model.add(Dense(units =1, activation='sigmoid'))\n",
        "\n",
        "  model.summary()\n",
        "  opt = args.optimizer\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  tf.keras.backend.set_value(model.optimizer.learning_rate, args.learning_rate)\n",
        "  return model\n",
        "\n",
        "\n",
        "def main(args):\n",
        "  # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
        "  # layers on each device across all workers\n",
        "  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
        "      communication=tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
        "  logging.debug(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
        "  BATCH_SIZE_PER_REPLICA = args.batch_size\n",
        "  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "  # Datasets need to be created after instantiation of `MultiWorkerMirroredStrategy`\n",
        "  train_dataset, test_dataset = make_datasets_unbatched()\n",
        "  train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
        "  test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)\n",
        "\n",
        "  # See: https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions\n",
        "  options = tf.data.Options()\n",
        "  options.experimental_distribute.auto_shard_policy = \\\n",
        "        tf.data.experimental.AutoShardPolicy.DATA\n",
        "\n",
        "  train_datasets_sharded  = train_dataset.with_options(options)\n",
        "  test_dataset_sharded = test_dataset.with_options(options)\n",
        "\n",
        "  with strategy.scope():\n",
        "    # Model building/compiling need to be within `strategy.scope()`.\n",
        "    multi_worker_model = model(args)\n",
        "\n",
        "  # Keras' `model.fit()` trains the model with specified number of epochs and\n",
        "  # number of steps per epoch. \n",
        "  multi_worker_model.fit(train_datasets_sharded,\n",
        "                         epochs=12,\n",
        "                         steps_per_epoch=5)\n",
        "  \n",
        "  eval_loss, eval_acc = multi_worker_model.evaluate(test_dataset_sharded, \n",
        "                                                    verbose=0, steps=10)\n",
        "\n",
        "  # Log metrics for Katib\n",
        "  logging.info(\"loss={:.4f}\".format(eval_loss))\n",
        "  logging.info(\"accuracy={:.4f}\".format(eval_acc))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--batch_size\",\n",
        "                      type=int,\n",
        "                      default=12,\n",
        "                      metavar=\"N\",\n",
        "                      help=\"Batch size for training (default: 128)\")\n",
        "  parser.add_argument(\"--learning_rate\", \n",
        "                      type=float,  \n",
        "                      default=0.001,\n",
        "                      metavar=\"N\",\n",
        "                      help='Initial learning rate')\n",
        "  parser.add_argument(\"--optimizer\", \n",
        "                      type=str, \n",
        "                      default='adam',\n",
        "                      metavar=\"N\",\n",
        "                      help='optimizer')\n",
        "\n",
        "  parsed_args, _ = parser.parse_known_args()\n",
        "  main(parsed_args)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting tfjobtitan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpjpKFy7z6wH"
      },
      "source": [
        "That saves the file as defined by `TRAINER_FILE` but it does not run it.\n",
        "\n",
        "Let's see if our code is correct by running it from within our notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31H9dsP4z6wI",
        "outputId": "727f5c2f-8807-41ca-c76b-97e0aad9c9a5"
      },
      "source": [
        "%run $TRAINER_FILE --optimizer 'sgd'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/tfjobtitan.py:94: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use distribute.MultiWorkerMirroredStrategy instead\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
            "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 20)                280       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 301\n",
            "Trainable params: 301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "5/5 [==============================] - 3s 2ms/step - loss: 0.6688 - accuracy: 0.8199\n",
            "Epoch 2/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 2.0207 - accuracy: 0.5961\n",
            "Epoch 3/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 1.5729 - accuracy: 0.6505\n",
            "Epoch 4/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.9189 - accuracy: 0.6688\n",
            "Epoch 5/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.8052 - accuracy: 0.5722\n",
            "Epoch 6/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 1.0549 - accuracy: 0.5116\n",
            "Epoch 7/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.7897 - accuracy: 0.5414\n",
            "Epoch 8/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6033 - accuracy: 0.7537\n",
            "Epoch 9/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.9174 - accuracy: 0.5366\n",
            "Epoch 10/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.9558 - accuracy: 0.5035\n",
            "Epoch 11/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.7531 - accuracy: 0.6211\n",
            "Epoch 12/12\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.7771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loss=0.7736\n",
            "INFO:root:accuracy=0.6583\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "trainer_dockerfile"
        ],
        "id": "C_t2_vOEz6wJ"
      },
      "source": [
        "## How to Create a Docker Image Manually\n",
        "\n",
        "\n",
        "The Dockerfile looks as follows:\n",
        "\n",
        "```\n",
        "  \n",
        "FROM tensorflow/tensorflow:2.4.0\n",
        "RUN pip install tensorflow_datasets pandas scikit-learn keras\n",
        "COPY tfjobchurn.py /\n",
        "ENTRYPOINT [\"python\", \"/tfjobchurn.py\", \"--batch_size\", \"10\", \"--learning_rate\", \"0.001\", \"--optimizer\", \"sgd\"]\n",
        "```\n",
        "\n",
        "\n",
        "Then it's easy to push images to your container registry:\n",
        "\n",
        "```bash\n",
        "docker build -t <docker_image_name_with_tag> .\n",
        "docker push <docker_image_name_with_tag>\n",
        "```\n",
        "\n",
        "The image is available as `mavencodev/tf_jobtitanic:1.0` in case you want to skip it for now.\n",
        "\n",
        "## How to Create a Distributed `TFJob`\n",
        "For large training jobs, we wish to run our trainer in a distributed mode.\n",
        "Once the notebook server cluster can access the Docker image from the registry, we can launch a distributed PyTorch job.\n",
        "\n",
        "The specification for a distributed `TFJob` is defined using YAML:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHcsIqTZz6wK",
        "outputId": "b83d5abb-e087-4310-e8b1-43247224b836"
      },
      "source": [
        "%%writefile $KUBERNETES_FILE\n",
        "apiVersion: \"kubeflow.org/v1\"\n",
        "kind: \"TFJob\"\n",
        "metadata:\n",
        "  name: \"churn\"\n",
        "  namespace: demo01 # your-user-namespace\n",
        "spec:\n",
        "  cleanPodPolicy: None\n",
        "  tfReplicaSpecs:\n",
        "    Worker:\n",
        "      replicas: 2\n",
        "      restartPolicy: OnFailure\n",
        "      template:\n",
        "        metadata:\n",
        "          annotations:\n",
        "            sidecar.istio.io/inject: \"false\"\n",
        "        spec:\n",
        "          containers:\n",
        "          - name: tensorflow\n",
        "            # modify this property if you would like to use a custom image\n",
        "            image: mavencodev/tf_jobtitanic:1.0\n",
        "            command:\n",
        "                - \"python\"\n",
        "                - \"/tfjobchurn.py\"\n",
        "                - \"--batch_size=10\"\n",
        "                - \"--learning_rate=0.001\"\n",
        "                - \"--optimizer=sgd\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting tfjob-titan.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmtxPwHHz6wM"
      },
      "source": [
        "Let's deploy the distributed training job:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlA1WSXvz6wM"
      },
      "source": [
        "%%capture tf_output --no-stderr\n",
        "! kubectl create -f $KUBERNETES_FILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "651dbwFpz6wN"
      },
      "source": [
        "TF_JOB = get_resource(tf_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlE0S9gSz6wO"
      },
      "source": [
        "To see the job status, use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmyyQvjQz6wP",
        "outputId": "9e060bab-15b6-4588-ea44-f87c3346fac0"
      },
      "source": [
        "! kubectl describe $TF_JOB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name:         churn\r\n",
            "Namespace:    demo01\r\n",
            "Labels:       <none>\r\n",
            "Annotations:  <none>\r\n",
            "API Version:  kubeflow.org/v1\r\n",
            "Kind:         TFJob\r\n",
            "Metadata:\r\n",
            "  Creation Timestamp:  2021-03-23T03:01:50Z\r\n",
            "  Generation:          1\r\n",
            "  Managed Fields:\r\n",
            "    API Version:  kubeflow.org/v1\r\n",
            "    Fields Type:  FieldsV1\r\n",
            "    fieldsV1:\r\n",
            "      f:spec:\r\n",
            "        .:\r\n",
            "        f:cleanPodPolicy:\r\n",
            "        f:tfReplicaSpecs:\r\n",
            "          .:\r\n",
            "          f:Worker:\r\n",
            "            .:\r\n",
            "            f:replicas:\r\n",
            "            f:restartPolicy:\r\n",
            "            f:template:\r\n",
            "              .:\r\n",
            "              f:metadata:\r\n",
            "                .:\r\n",
            "                f:annotations:\r\n",
            "                  .:\r\n",
            "                  f:sidecar.istio.io/inject:\r\n",
            "              f:spec:\r\n",
            "    Manager:      kubectl\r\n",
            "    Operation:    Update\r\n",
            "    Time:         2021-03-23T03:01:50Z\r\n",
            "    API Version:  kubeflow.org/v1\r\n",
            "    Fields Type:  FieldsV1\r\n",
            "    fieldsV1:\r\n",
            "      f:spec:\r\n",
            "        f:successPolicy:\r\n",
            "        f:tfReplicaSpecs:\r\n",
            "          f:Worker:\r\n",
            "            f:template:\r\n",
            "              f:metadata:\r\n",
            "                f:creationTimestamp:\r\n",
            "              f:spec:\r\n",
            "                f:containers:\r\n",
            "      f:status:\r\n",
            "        .:\r\n",
            "        f:conditions:\r\n",
            "        f:replicaStatuses:\r\n",
            "          .:\r\n",
            "          f:Worker:\r\n",
            "            .:\r\n",
            "            f:active:\r\n",
            "            f:succeeded:\r\n",
            "        f:startTime:\r\n",
            "    Manager:         tf-operator.v1\r\n",
            "    Operation:       Update\r\n",
            "    Time:            2021-03-23T03:02:40Z\r\n",
            "  Resource Version:  604610\r\n",
            "  Self Link:         /apis/kubeflow.org/v1/namespaces/demo01/tfjobs/churn\r\n",
            "  UID:               b46d4917-486a-4245-b39e-99a139af5950\r\n",
            "Spec:\r\n",
            "  Clean Pod Policy:  None\r\n",
            "  Tf Replica Specs:\r\n",
            "    Worker:\r\n",
            "      Replicas:        2\r\n",
            "      Restart Policy:  OnFailure\r\n",
            "      Template:\r\n",
            "        Metadata:\r\n",
            "          Annotations:\r\n",
            "            sidecar.istio.io/inject:  false\r\n",
            "        Spec:\r\n",
            "          Containers:\r\n",
            "            Command:\r\n",
            "              python\r\n",
            "              /tfjobchurn.py\r\n",
            "              --batch_size=64\r\n",
            "              --learning_rate=0.1\r\n",
            "              --optimizer=adam\r\n",
            "            Image:  mavencodev/tf_jobchurn:1.0\r\n",
            "            Name:   tensorflow\r\n",
            "Status:\r\n",
            "  Conditions:\r\n",
            "    Last Transition Time:  2021-03-23T03:01:50Z\r\n",
            "    Last Update Time:      2021-03-23T03:01:50Z\r\n",
            "    Message:               TFJob churn is created.\r\n",
            "    Reason:                TFJobCreated\r\n",
            "    Status:                True\r\n",
            "    Type:                  Created\r\n",
            "    Last Transition Time:  2021-03-23T03:02:26Z\r\n",
            "    Last Update Time:      2021-03-23T03:02:26Z\r\n",
            "    Message:               TFJob churn is running.\r\n",
            "    Reason:                TFJobRunning\r\n",
            "    Status:                True\r\n",
            "    Type:                  Running\r\n",
            "  Replica Statuses:\r\n",
            "    Worker:\r\n",
            "      Active:     1\r\n",
            "      Succeeded:  1\r\n",
            "  Start Time:     2021-03-23T03:01:50Z\r\n",
            "Events:\r\n",
            "  Type    Reason                   Age              From         Message\r\n",
            "  ----    ------                   ----             ----         -------\r\n",
            "  Normal  SuccessfulCreatePod      50s              tf-operator  Created pod: churn-worker-0\r\n",
            "  Normal  SuccessfulCreatePod      50s              tf-operator  Created pod: churn-worker-1\r\n",
            "  Normal  SuccessfulCreateService  50s              tf-operator  Created service: churn-worker-0\r\n",
            "  Normal  SuccessfulCreateService  50s              tf-operator  Created service: churn-worker-1\r\n",
            "  Normal  ExitedWithCode           0s (x2 over 0s)  tf-operator  Pod: demo01.churn-worker-1 exited with code 0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgbQ_hyfz6wQ"
      },
      "source": [
        "You should now be able to see the created pods matching the specified number of workers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USVwyRqkz6wQ",
        "outputId": "90ad0257-23f0-4243-cc3b-9cb8cbe6e660"
      },
      "source": [
        "! kubectl get pods -l job-name=churn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME             READY   STATUS      RESTARTS   AGE\r\n",
            "churn-worker-0   0/1     Completed   0          60s\r\n",
            "churn-worker-1   0/1     Completed   0          60s\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqITItC3z6wR"
      },
      "source": [
        "In case of issues, it may be helpful to see the last ten events within the cluster:\n",
        "\n",
        "```bash\n",
        "! kubectl get events --sort-by='.lastTimestamp' | tail\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj1hTxkzz6wR"
      },
      "source": [
        "To stream logs from the worker-0 pod to check the training progress, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAP1jEt2z6wS",
        "outputId": "2d0d7ac7-4f1c-4da8-974a-5c142c232480"
      },
      "source": [
        "! kubectl logs -f churn-worker-0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-23 03:02:26.525110: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
            "2021-03-23 03:02:26.525145: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
            "WARNING:tensorflow:From /tfjobchurn.py:76: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\r\n",
            "Instructions for updating:\r\n",
            "use distribute.MultiWorkerMirroredStrategy instead\r\n",
            "2021-03-23 03:02:28.311728: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
            "2021-03-23 03:02:28.311957: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n",
            "2021-03-23 03:02:28.311979: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
            "2021-03-23 03:02:28.312006: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (churn-worker-0): /proc/driver/nvidia/version does not exist\r\n",
            "2021-03-23 03:02:28.313325: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
            "2021-03-23 03:02:28.314210: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n",
            "2021-03-23 03:02:28.320870: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> churn-worker-0.demo01.svc:2222, 1 -> churn-worker-1.demo01.svc:2222}\r\n",
            "2021-03-23 03:02:28.321521: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://churn-worker-0.demo01.svc:2222\r\n",
            "INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']\r\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0',)\r\n",
            "INFO:tensorflow:Waiting for the cluster, timeout = inf\r\n",
            "INFO:tensorflow:Cluster is ready.\r\n",
            "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['churn-worker-0.demo01.svc:2222', 'churn-worker-1.demo01.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO\r\n",
            "2021-03-23 03:02:29.670155: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n",
            "2021-03-23 03:02:29.670584: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300070000 Hz\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "Model: \"sequential\"\r\n",
            "_________________________________________________________________\r\n",
            "Layer (type)                 Output Shape              Param #   \r\n",
            "=================================================================\r\n",
            "dense (Dense)                (None, 9)                 108       \r\n",
            "_________________________________________________________________\r\n",
            "dense_1 (Dense)              (None, 9)                 90        \r\n",
            "_________________________________________________________________\r\n",
            "dense_2 (Dense)              (None, 1)                 10        \r\n",
            "=================================================================\r\n",
            "Total params: 208\r\n",
            "Trainable params: 208\r\n",
            "Non-trainable params: 0\r\n",
            "_________________________________________________________________\r\n",
            "Epoch 1/50\r\n",
            "30/30 [==============================] - 3s 3ms/step - loss: 1298.5505 - accuracy: 0.7001\r\n",
            "Epoch 2/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4954 - accuracy: 0.8049\r\n",
            "Epoch 3/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4996 - accuracy: 0.8008\r\n",
            "Epoch 4/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5109 - accuracy: 0.7925\r\n",
            "Epoch 5/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5241 - accuracy: 0.7826\r\n",
            "Epoch 6/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5099 - accuracy: 0.7933\r\n",
            "Epoch 7/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5206 - accuracy: 0.7853\r\n",
            "Epoch 8/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5066 - accuracy: 0.7959\r\n",
            "Epoch 9/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5047 - accuracy: 0.7972\r\n",
            "Epoch 10/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4978 - accuracy: 0.8028\r\n",
            "Epoch 11/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5046 - accuracy: 0.7970\r\n",
            "Epoch 12/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5043 - accuracy: 0.7976\r\n",
            "Epoch 13/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5095 - accuracy: 0.7938\r\n",
            "Epoch 14/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5004 - accuracy: 0.8001\r\n",
            "Epoch 15/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4989 - accuracy: 0.8012\r\n",
            "Epoch 16/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4978 - accuracy: 0.8026\r\n",
            "Epoch 17/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5238 - accuracy: 0.7827\r\n",
            "Epoch 18/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5162 - accuracy: 0.7886\r\n",
            "Epoch 19/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5118 - accuracy: 0.7924\r\n",
            "Epoch 20/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5112 - accuracy: 0.7929\r\n",
            "Epoch 21/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4967 - accuracy: 0.8027\r\n",
            "Epoch 22/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5108 - accuracy: 0.7926\r\n",
            "Epoch 23/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.4903 - accuracy: 0.8074\r\n",
            "Epoch 24/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4848 - accuracy: 0.8130\r\n",
            "Epoch 25/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7900\r\n",
            "Epoch 26/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4999 - accuracy: 0.8006\r\n",
            "Epoch 27/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4948 - accuracy: 0.8046\r\n",
            "Epoch 28/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4978 - accuracy: 0.8022\r\n",
            "Epoch 29/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5177 - accuracy: 0.7870\r\n",
            "Epoch 30/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5024 - accuracy: 0.7993\r\n",
            "Epoch 31/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5027 - accuracy: 0.7986\r\n",
            "Epoch 32/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7976\r\n",
            "Epoch 33/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5008 - accuracy: 0.8002\r\n",
            "Epoch 34/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5140 - accuracy: 0.7914\r\n",
            "Epoch 35/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5083 - accuracy: 0.7945\r\n",
            "Epoch 36/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4966 - accuracy: 0.8028\r\n",
            "Epoch 37/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5004 - accuracy: 0.8008\r\n",
            "Epoch 38/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4865 - accuracy: 0.8105\r\n",
            "Epoch 39/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5053 - accuracy: 0.7965\r\n",
            "Epoch 40/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5019 - accuracy: 0.7993\r\n",
            "Epoch 41/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4904 - accuracy: 0.8076\r\n",
            "Epoch 42/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4908 - accuracy: 0.8072\r\n",
            "Epoch 43/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5167 - accuracy: 0.7883\r\n",
            "Epoch 44/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5096 - accuracy: 0.7937\r\n",
            "Epoch 45/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5330 - accuracy: 0.7772\r\n",
            "Epoch 46/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4961 - accuracy: 0.8029\r\n",
            "Epoch 47/50\r\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.8001\r\n",
            "Epoch 48/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5022 - accuracy: 0.7996\r\n",
            "Epoch 49/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5236 - accuracy: 0.7852\r\n",
            "Epoch 50/50\r\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5333 - accuracy: 0.7774\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\n",
            "INFO:root:loss=0.5346\r\n",
            "INFO:root:accuracy=0.7820\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhznskZ8z6wS"
      },
      "source": [
        "To delete the job, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYl6-9mtz6wT",
        "outputId": "6117035c-7284-452f-fe77-bfde958b81ec"
      },
      "source": [
        "! kubectl delete $TF_JOB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfjob.kubeflow.org \"churn\" deleted\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk_Uf3Xfz6wU"
      },
      "source": [
        "Check to see if the check to see if the pod is still up and running "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi4lyWraz6wU"
      },
      "source": [
        "! kubectl -n demo01 logs -f churn"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}